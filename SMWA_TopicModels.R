# Set script -----------------------------------------------------------
rm(list=ls())

library(rstudioapi)
#sets working directory to file directory
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))

#Load the required packages to perform topic modeling
if (!require("pacman")) install.packages("pacman") ; require("pacman")
if (!require("fastDummies")) install.packages("fastDummies") ; require("fastDummies")
p_load(rtweet, httr,tidyverse,wordcloud, tm, topicmodels, tidytext, textclean, fastDummies)

# Get dataset_cleaned -----------------------------------------------------

data_clean <- read_csv('dataset_cleaned.csv')

# Topic Modeling ----------------------------------------------------------

#First create a tibble 
text_df <- tibble(doc= 1:length(data_clean$text), text = data_clean$text)

#Next, make a word frequency table
freq <- text_df %>%
  unnest_tokens(word, text) %>% #tokenize (split documents into terms)
  anti_join(stop_words) %>% #Remove stopwords
  count(doc,word, name = "freq", sort = TRUE)

#Cast dtm from this word count table
dtm <- freq %>%
  cast_dtm(doc, word, freq)

#This actually creates a tm object
dtm
inspect(dtm)

#We must know the optimal number of topics K
#Iterate k over a number of values  
#Use the AIC to select the best model (lower is better)
ldas <- list()
j <- 0
for (i in 2:20) {
  j <- j+1
  print(i)
  #We set a seed for the LDA algorithm such that the results are predictable and comparable
  #This uses the VEM optimization algorithm as defined by the inventor (Blei)
  #You can also choose to perform Gibbs sampling (method option)
  ldas[[j]] <- LDA(x = dtm, k = i, control = list(seed = 1234))
}

(AICs <- data.frame(k = 2:20, aic = sapply(ldas, AIC)))
(K <- AICs$k[which.min(AICs$aic)])

# Note that the K with the best performance is 2 (lowest AIC)
# But the topics are not very meaningfull and we want more predictors for the model
# Therefore we use a K of 4
# The AIC difference between K = 2 and K = 4 is also reasonably small

#Make final LDA
topicmodel <- LDA(x = dtm, k = K + 2, control = list(seed = 1234))

###### Topics and terms
(topic_term <- tidy(topicmodel, matrix = 'beta'))
# each line represents a topic per term

#Have a look at the top 10 terms per topics
top_terms <- topic_term %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, desc(beta))
top_terms

#Plot the top terms per topic
top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()

#Make a worldcloud per topic
p_load(reshape2, BiocManager, RColorBrewer)

topic_term %>%
  group_by(topic) %>%
  top_n(50, beta) %>%
  acast(term ~ topic, value.var = "beta", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(3, "Set2"))

###### Topics and documents

#Get per document per topic matrix
(doc_topic <- tidy(topicmodel, matrix = 'gamma'))
#E.g., X% of the words in document 1 are generated by topic 1

#Get the top topic for each document 
(topics_gamma <- doc_topic %>% arrange(desc(gamma)))

user_topic <- topics_gamma %>%
  group_by(document) %>%
  top_n(1, gamma)

#Now add the original tweets to see what the topic are about
user_topic_tweet <- user_topic %>% 
  add_column(Tweets = data_clean$text[as.numeric(user_topic$document)])


# Create final dataset ----------------------------------------------------

# Create predictors as dumies
user_topic_tweet <- fastDummies::dummy_cols(user_topic_tweet, select_columns = "topic")

# Create predictors as percentages
doc_topic <- fastDummies::dummy_cols(doc_topic, select_columns = "topic")

# Very slow code, needs to be rewritten so it works faster
for (i in 1:nrow(doc_topic)) {
  doc_topic[i,"topic_1"] = doc_topic[i,"topic_1"]*doc_topic[i,"gamma"]
  doc_topic[i,"topic_2"] = doc_topic[i,"topic_2"]*doc_topic[i,"gamma"]
  doc_topic[i,"topic_3"] = doc_topic[i,"topic_3"]*doc_topic[i,"gamma"]
  doc_topic[i,"topic_4"] = doc_topic[i,"topic_4"]*doc_topic[i,"gamma"]
}




